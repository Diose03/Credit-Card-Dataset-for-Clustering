# -*- coding: utf-8 -*-
"""Tarea_04.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iiy4yE1Vt6FMg4JXX9dEOCX5bezc6qot

#**Credit Card Dataset for Clustering**

---



---

<br>

<div style="text-align: justify;">

Este caso requiere desarrollar una segmentación de clientes para definir la estrategia de marketing.

El conjunto de datos de muestra resume el comportamiento de uso de aproximadamente 9000 titulares de tarjetas de crédito activos durante los últimos 6 meses. El archivo está a nivel de cliente con 18 variables de comportamiento.

</div>

<div style="text-align: justify;">

A continuación se muestra el diccionario de datos del conjunto de datos de tarjetas de crédito:

* CUST_ID : Identificación del titular.

* BALANCE : Saldo restante.

* BALANCE_FREQUENCY : Frecuencia con la que se actualiza el saldo.

* PURCHASES : Cantidad de compras realizadas desde la cuenta.

* ONEOFF_PURCHASES : Cantidad máxima de compras realizadas en una sola vez.

* INSTALLMENTS_PURCHASES : Cantidad de compras realizadas en cuotas.

* CASH_ADVANCE : Anticipo en efectivo otorgado por el usuario.

* PURCHASES_FREQUENCY : Frecuencia con la que se realizan las compras.

* ONEOFFPURCHASES: Frecuencia con la que se realizan las compras en una sola vez.

* PURCHASESINSTALLMENTSFREQUENCY : Frecuencia con la que se realizan las compras en cuotas hecho.

* CASHADVANCEFREQUENCY : Con qué frecuencia se paga el efectivo por adelantado.

* CASHADVANCETRX : Número de transacciones realizadas con "Efectivo por adelantado".

* PURCHASES_TRX : Número de transacciones de compra realizadas.

* CREDIT_LIMIT : Límite de tarjeta de crédito para el usuario.

* PAYMENTS : Monto del pago realizado por el usuario.

* MINIMUM_PAYMENTS : Monto mínimo de pagos realizados por el usuario.

* PRCFULLPAYMENT : Porcentaje del pago total pagado por el usuario.

* TENURE : Duración del servicio de tarjeta de crédito para el usuario.

</div>

<br>

---



---
"""

"""*   **Importación de librerías**






"""

import random
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans

"""* **Cargamos la BD**"""

# Leemos la base de datos
datos = pd.read_csv('CC GENERAL.csv')

#Leemos los primero 5 datos que contiene.
datos.head(5)

"""* **Resumen del dataframe**"""

datos.info()

"""* **Limpieza de datos.**"""

# Eliminamos las columnas innecesarias pues para los cálculos no es conveniente tener palabras.
data = datos.drop(columns=['CUST_ID'])
data.head()

"""* **Verificamos si existen datos nulos**"""

datos.isnull().sum()

"""* **Llenar los valores faltantes**"""

# Rellenar los valores faltantes con la mediana
data['MINIMUM_PAYMENTS'] = data['MINIMUM_PAYMENTS'].fillna(data['MINIMUM_PAYMENTS'].median())
data['CREDIT_LIMIT'] = data['CREDIT_LIMIT'].fillna(data['CREDIT_LIMIT'].median())

"""* **Análisis de outliers con Rango Intercuartílico (IQR)**"""

# Calcular IQR para cada columna relevante
columns_to_check = ['BALANCE', 'PURCHASES', 'ONEOFF_PURCHASES', 'CASH_ADVANCE', 'CREDIT_LIMIT', 'MINIMUM_PAYMENTS']
outliers = {}

for col in columns_to_check:
    Q1 = data[col].quantile(0.25)
    Q3 = data[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers[col] = data[(data[col] < lower_bound) | (data[col] > upper_bound)]

"""* **Diagrama de cajas**"""

plt.figure(figsize=(15, 8))
sns.boxplot(data=data[columns_to_check], orient="h")
plt.title("Diagrama de cajas de columnas con potenciales valores atípicos")
plt.show()

"""* **Eliminamos los outliers con valores atípicos significativos**"""

columnas_outliers = ['PURCHASES', 'CASH_ADVANCE', 'MINIMUM_PAYMENTS', 'ONEOFF_PURCHASES', 'CREDIT_LIMIT']

# Función para eliminar outliers utilizando IQR
def eliminar_outliers_iqr(df, columnas):
    for columna in columnas:
        Q1 = df[columna].quantile(0.25)  # primer cuartil
        Q3 = df[columna].quantile(0.75)  # tercer cuartil
        IQR = Q3 - Q1  # Rango intercuartílico

        # Definimos los límites inferior y superior
        limite_inferior = Q1 - 1.5 * IQR
        limite_superior = Q3 + 1.5 * IQR

        # Filtramos los datos dentro de los límites
        df = df[(df[columna] >= limite_inferior) & (df[columna] <= limite_superior)]

    return df

columnas_a_normalizar = eliminar_outliers_iqr(data, columnas_outliers)
print(f"Filas antes de eliminar outliers: {data.shape[0]}")
print(f"Filas después de eliminar outliers: {columnas_a_normalizar.shape[0]}")

"""* **Normalizamos los datos**"""

# Escalamos los datos y normalizamos solo las columnas seleccionadas
scaler = StandardScaler()

columnas_a_normalizar[columnas_outliers] = scaler.fit_transform(columnas_a_normalizar[columnas_outliers])
columnas_a_normalizar.head()

"""* **Aplicamos PCA**"""

# Modelo PCA ajustado a los datos normalizados
pca = PCA(n_components=2)  # Asignamos 2 componentes principales
componentes_principales = pca.fit_transform(columnas_a_normalizar[columnas_outliers])

# Varianza por cada componente
print("Varianza explicada por cada componente principal:", pca.explained_variance_ratio_)
print("Varianza total explicada:", sum(pca.explained_variance_ratio_))

"""* **Graficamos los resultados de PCA para visualizar la reducción de dimensionalidad**"""

plt.figure(figsize=(8, 6))
plt.scatter(componentes_principales[:, 0], componentes_principales[:, 1])
plt.title("PCA - Proyección de los Datos Normalizados")
plt.xlabel("Componente 1")
plt.ylabel("Componente 2")
plt.show()

"""* **Revisamos los datos de nuevo, esta vez con nuestros componentes pincipales.**

"""

columnas_pca = ['PCA1', 'PCA2']  # Nombres para las componentes principales
pca = pd.DataFrame(data=componentes_principales, columns=columnas_pca)
pca.head()

"""* **Ahora aplicaremos Kmeans.**

> Para este problema es conveniente aplicar el Método del Codo para obtener un número aproximado de clústeres, seguido de la evaluación con el Coeficiente de la Silueta para asegurar que la segmentación sea de buena calidad.
"""

# Calcular la inercia para diferentes valores de K
inercia = []
silhouette_scores = []
K = range(1, 10)

for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(pca[['PCA1', 'PCA2']])
    inercia.append(kmeans.inertia_)

    # Calculamos el coeficiente de silueta solo si hay más de 1 cluster
    if k > 1:
        score = silhouette_score(pca[['PCA1', 'PCA2']], kmeans.labels_)
        silhouette_scores.append(score)

# Método del codo
plt.figure(figsize=(8, 6))
plt.plot(K, inercia, marker='o', linestyle='--')
plt.xlabel('Número de clusters (K)')
plt.ylabel('Inercia')
plt.title('Método del codo')
plt.show()

# Coeficiente de silueta
if silhouette_scores:
    plt.figure(figsize=(8, 6))
    plt.plot(range(2, 10), silhouette_scores, marker='o', linestyle='--', color='orange')
    plt.xlabel('Número de clusters (K)')
    plt.ylabel('Coeficiente de Silueta')
    plt.title('Coeficiente de Silueta')
    plt.show()

"""* **Podemos observar que el punto alto para nuestros clusters es k = 3**"""

# 3 clusters y un valor fijo para la semilla aleatoria
kmeans_final = KMeans(n_clusters=3, random_state=42)
kmeans_final.fit(pca[['PCA1', 'PCA2']])
etiquetas = kmeans_final.labels_

# Agregamos las etiquetas al DataFrame original
pca['Cluster'] = etiquetas

print("Asignación de clusters:")
for index, row in pca.iterrows():
    print(f"Punto {index}: Cluster {row['Cluster']}, Coordenadas PCA1: {row['PCA1']:.2f}, PCA2: {row['PCA2']:.2f}")

# Matriz de correlación
corr_matrix = data.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Matriz de Correlación')
plt.show()

"""* **Resultados de la segmentación**"""

plt.figure(figsize=(8, 6))
for cluster in range(3):  # Para cada cluster
    cluster_data = pca[pca['Cluster'] == cluster]
    plt.scatter(cluster_data['PCA1'], cluster_data['PCA2'], label=f'Cluster {cluster}')

plt.xlabel('PCA1')
plt.ylabel('PCA2')
plt.title('Clusters en el espacio PCA')
plt.legend()
plt.show()

"""* **Para crear los clusters finales aplicaremos Algoritmo Genético**"""

# Solución inicial
def sol_inicial():
    return list(kmeans_final.labels_)

# Función objetivo
def func_obj(solucion, obs):
    centroides = [[0, 0] for _ in range(3)]
    contador = [0] * 3

    # Sumamos las coordenadas de los puntos a cada centroide según su cluster
    for i in range(len(obs)):
        cluster = solucion[i]
        centroides[cluster] = [x + y for x, y in zip(centroides[cluster], obs[i])]
        contador[cluster] += 1

    for i in range(3):
        if contador[i] > 0:
            centroides[i] = [x / contador[i] for x in centroides[i]]

    # Calcular la distancia Manhattan entre cada punto y su centroide
    distancia = 0
    for i in range(len(obs)):
        cluster = solucion[i]
        distancia += sum(abs(centroides[cluster][j] - obs[i][j]) for j in range(2))

    return distancia

# Selección de padres
def seleccion_padres(tam_pob, costos):
    p_seleccion = 0.9
    elegidos = []
    barajeado = random.sample(range(tam_pob), tam_pob)

    for i in range(tam_pob // 2):
        j = barajeado[2 * i]
        k = barajeado[2 * i + 1]
        min_idx, max_idx = (j, k) if costos[j] < costos[k] else (k, j)
        b = random.random()
        if b < p_seleccion:
            elegidos.append(min_idx)
        else:
            elegidos.append(max_idx)

    while len(elegidos) < tam_pob:
        elegidos.append(random.choice(elegidos))

    return elegidos

# Algoritmo Genético
def algoritmo_genetico(obs, imprimir=False):
    tam_pob = 10  # Tamaño de la población
    tam_individuo = len(obs)  # Tamaño de cada individuo
    generaciones = 200  # Número de generaciones
    p = 0.9  # Probabilidad de cruzamiento
    m = 0.01  # Probabilidad de mutación

    poblacion = [[0 for _ in range(tam_individuo)] for _ in range(tam_pob)]
    costos = [0.0 for _ in range(tam_pob)]
    mejor_sol = [0 for _ in range(tam_individuo)]
    mejor_costo = float('inf')

    # Generar Población inicial
    for i in range(tam_pob):
        poblacion[i] = sol_inicial()  # Usamos las etiquetas de KMeans como la población inicial
        costos[i] = func_obj(poblacion[i], obs)

        if costos[i] < mejor_costo:
            mejor_costo = costos[i]
            mejor_sol[:] = poblacion[i][:]

    # Algoritmo Genético
    for it in range(generaciones):
        padres = seleccion_padres(tam_pob, costos)

        hijos = [[0 for _ in range(tam_individuo)] for _ in range(tam_pob)]

        # Cruzamiento
        for i in range(tam_pob // 2):
            p1, p2 = padres[i], padres[i + tam_pob // 2]
            for j in range(tam_individuo):
                b = random.random()
                if b < p:
                    hijos[2 * i][j] = poblacion[p1][j]
                    hijos[2 * i + 1][j] = poblacion[p2][j]
                else:
                    hijos[2 * i][j] = poblacion[p2][j]
                    hijos[2 * i + 1][j] = poblacion[p1][j]

        # Mutación
        for i in range(tam_pob):
            for j in range(tam_individuo):
                if random.random() < m:
                    hijos[i][j] = (hijos[i][j] + random.choice([-1, 1])) % 3  # Cambiamos el cluster asignado

        # Reemplazo generacional
        for i in range(tam_pob):
            poblacion[i] = hijos[i][:]  # Los hijos reemplazan a los padres
            costos[i] = func_obj(poblacion[i], obs)  # Calculamos el costo de los hijos
            if costos[i] < mejor_costo:
                mejor_costo = costos[i]
                mejor_sol[:] = poblacion[i][:]

    return mejor_sol, mejor_costo

mejor_sol, mejor_costo = algoritmo_genetico(pca[['PCA1', 'PCA2']].values, imprimir=True)

#print(f"Mejor solución encontrada (clusters asignados): {mejor_sol}")
print(f"Mejor costo (distancia de Manhattan total): {mejor_costo}")

# Solución inicial
def sol_inicial(kmeans_labels):
    return list(kmeans_labels)

# Función objetivo
def func_obj(solucion, obs):
    clusters = np.unique(solucion)
    centroides = np.zeros((len(clusters), obs.shape[1]))

    # Calculamos los centroides
    for cluster in clusters:
        puntos_cluster = obs[np.array(solucion) == cluster]
        centroides[cluster] = np.mean(puntos_cluster, axis=0)

    # Calculamos la distancia total Euclidiana
    distancia = sum(
        np.linalg.norm(obs[i] - centroides[solucion[i]])
        for i in range(len(obs))
    )
    return distancia

# Realizamos la selección de padres
def seleccion_padres(costos, tam_pob):
    padres = []
    for _ in range(tam_pob):
        competidores = random.sample(range(len(costos)), 2)
        mejor = min(competidores, key=lambda x: costos[x])
        padres.append(mejor)
    return padres

# Algoritmo Genético
def algoritmo_genetico(obs, kmeans_labels, imprimir=False):
    tam_pob = 10  # Tamaño de la población
    generaciones = 200  # Número de generaciones
    prob_cruzamiento = 0.9  # Probabilidad de cruzamiento
    prob_mutacion = 0.01  # Probabilidad de mutación

    poblacion = [sol_inicial(kmeans_labels) for _ in range(tam_pob)]
    costos = [func_obj(ind, obs) for ind in poblacion]

    mejor_sol = poblacion[np.argmin(costos)]
    mejor_costo = min(costos)

    # Algoritmo Genético
    for _ in range(generaciones):
        padres_idx = seleccion_padres(costos, tam_pob)
        nueva_poblacion = []

        # Cruzamiento
        for i in range(0, tam_pob, 2):
            p1, p2 = poblacion[padres_idx[i]], poblacion[padres_idx[i + 1]]
            hijo1, hijo2 = cruzamiento(p1, p2, prob_cruzamiento)
            nueva_poblacion.extend([hijo1, hijo2])

        # Mutación
        for i in range(tam_pob):
            nueva_poblacion[i] = mutacion(nueva_poblacion[i], prob_mutacion)

        # Reemplazo y actualización de mejores soluciones
        costos = [func_obj(ind, obs) for ind in nueva_poblacion]
        poblacion = nueva_poblacion
        gen_mejor_costo = min(costos)
        if gen_mejor_costo < mejor_costo:
            mejor_costo = gen_mejor_costo
            mejor_sol = poblacion[np.argmin(costos)]

    return mejor_sol, mejor_costo

# Cruzamiento
def cruzamiento(p1, p2, prob):
    hijo1, hijo2 = p1[:], p2[:]
    if random.random() < prob:
        for i in range(len(p1)):
            if random.random() < 0.5:
                hijo1[i], hijo2[i] = hijo2[i], hijo1[i]
    return hijo1, hijo2

# Mutación
def mutacion(individuo, prob):
    for i in range(len(individuo)):
        if random.random() < prob:
            individuo[i] = random.choice([0, 1, 2])  # Suponiendo 3 clusters
    return individuo

kmeans_labels = kmeans_final.labels_
obs = pca[['PCA1', 'PCA2']].values

mejor_sol, mejor_costo = algoritmo_genetico(obs, kmeans_labels, imprimir=True)
#print(f"Mejor solución encontrada (clusters asignados): {mejor_sol}")
print(f"Mejor costo (distancia Euclidiana total): {mejor_costo}")

"""* **Una vez obtenida la mejor solución, visualizamos los clusters finales.**"""

# Convertir la mejor solución en un DataFrame para análisis
df_resultados = pd.DataFrame(obs, columns=['PCA1', 'PCA2'])
df_resultados['Cluster'] = mejor_sol  # Asignamos clusters según la mejor solución

def visualizar_clusters(df):
    plt.figure(figsize=(10, 6))
    sns.scatterplot(data=df, x='PCA1', y='PCA2', hue='Cluster', palette='viridis', s=50, style='Cluster', markers=["o", "s", "D"])
    plt.title("Distribución de las Soluciones Asignadas a los Clusters Finales", fontsize=16)
    plt.xlabel("PCA1", fontsize=12)
    plt.ylabel("PCA2", fontsize=12)
    plt.legend(title="Cluster", fontsize=10)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.show()

visualizar_clusters(df_resultados)

"""* **Resumen estadístico**"""

from tabulate import tabulate

def analizar_clusters(df):

    # Resumen descriptivo por cluster
    resumen = df.groupby('Cluster').agg({
        'PCA1': ['mean', 'std', 'min', 'max'],
        'PCA2': ['mean', 'std', 'min', 'max']
    })

    print("\nResumen estadístico por cluster:")
    print(tabulate(resumen, headers='keys', tablefmt='pretty'))

    conteo_clusters = df['Cluster'].value_counts().reset_index()
    conteo_clusters.columns = ['Cluster', 'Número de muestras']

    print("\nNúmero de muestras por cluster:")
    print(tabulate(conteo_clusters, headers='keys', tablefmt='pretty'))

analizar_clusters(df_resultados)

"""* **Características específicas dentro de cada grupo
que pueden ser útiles para una interpretación más profunda de los datos**
"""

# Asignar los clusters a los datos originales
df_resultados['Cluster'] = mejor_sol

# Analizar las características de cada cluster
for cluster in np.unique(mejor_sol):
    cluster_data = df_resultados[df_resultados['Cluster'] == cluster]
    avg_pca1 = cluster_data['PCA1'].mean()
    avg_pca2 = cluster_data['PCA2'].mean()
    print(f"Cluster {cluster}: Promedio PCA1 = {avg_pca1:.2f}, Promedio PCA2 = {avg_pca2:.2f}")

# Ejemplo de asignación de estrategia de marketing basada en los clusters
for cluster in np.unique(mejor_sol):
    if cluster == 0:
        estrategia = "Campañas de alto valor, promociones exclusivas."
    elif cluster == 1:
        estrategia = "Incentivos para aumentar uso de crédito, productos complementarios."
    else:
        estrategia = "Ofrecer promociones de bajo riesgo y fidelización."

    print(f"Estrategia para el Cluster {cluster}: {estrategia}")

"""**Finalmente, las estrategias de marketing sugeridas son:**
* **Cluster 0**: Clientes con un perfil conservador, se recomienda campañas de alto valor.
* **Cluster 1**: Clientes activos con el crédito, se sugiere incentivos y productos complementarios.
* **Cluster 2**: Clientes estables, se propone promociones de bajo riesgo y fidelización.
"""